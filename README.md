# KANGuard: Investigating the Efficacy of Graph Neural Networks for Detecting AI-Generated Business Email Compromise (BEC) Attacks

## Introduction

This project explores the capabilities of hybrid architectures combining Graph Neural Networks (GNNs) and Kolmogorov-Arnold Networks (KANs) for the detection of Business Email Compromise (BEC) attacks. We specifically focus on evaluating these methods on a modern dataset where malicious emails are generated by Large Language Models (LLMs) like GPT.

The primary hypothesis of this research is that structural and metadata information within email communication networks can provide supplementary signals, enhancing detection performance beyond what is achievable with models that rely solely on text content analysis.

## Dataset

This project utilizes the dataset published by Dube et al. (2024), sourced from the [r-dube/bec](https://github.com/r-dube/bec) repository. The dataset comprises:
*   **BEC Emails (Malicious):** Emails generated by LLMs (GPT-3.5, GPT-4) simulating various BEC scenarios.
*   **HAM Emails (Benign):** A subset of legitimate emails from the public Enron corpus.

Initial preprocessing and exploratory data analysis are performed in the `01_explore_data.py` script, which generates the working dataset `analysis_outputs/combined_dataset_v2.csv`.

## Experimental Pipeline

The project is structured as a fully reproducible 5-step pipeline:

1.  **`01_explore_data.py`**: Loads the raw data, performs cleaning, merges the datasets, and conducts exploratory data analysis (EDA). The output is a clean CSV file and analytical plots.
2.  **`02_train_models.py`**: Trains and evaluates two critical baseline models:
    *   **Naive Bayes:** A simple baseline to establish the problem's difficulty.
    *   **DistilBERT:** A powerful text-only baseline representing the state-of-the-art in content-based classification.
3.  **`03_train_kangraph.py`**: Implements and evaluates the first version of our proposed model, **KANGuard-Sim**, which constructs a homogeneous graph based on content similarity between emails.
4.  **`04_build_hetero_graph.py`**: Constructs a more complex heterogeneous graph, modeling the relationships between three entity types: `Email`, `Sender`, and `Domain` (extracted from URLs).
5.  **`05_train_hetero_kangraph.py`**: Implements and evaluates the second version, **KANGuard-Hetero**, on the heterogeneous graph.

## Results & Key Findings

Our comprehensive experiments led to several important and somewhat surprising findings:

| Model                     | F1-Score (BEC Class) | Precision (BEC) | Recall (BEC) | Notes                                           |
| ------------------------- | -------------------- | --------------- | ------------ | ----------------------------------------------- |
| Naive Bayes               | 0.00                 | 0.00            | 0.00         | Fails completely; only predicts the majority class. |
| DistilBERT (Text-only)    | **0.52**             | **0.35**        | **1.00**     | Strong baseline; identifies all BEC emails.       |
| KANGuard-Sim (Homogeneous)| **0.52**             | **0.35**        | **1.00**     | Performance is identical to DistilBERT.           |
| KANGuard-Hetero           | **0.52**             | **0.35**        | **1.00**     | Performance is identical to DistilBERT.           |

**Primary Finding:**

> For this specific AI-generated BEC dataset, the decisive classification signals are almost entirely contained within the semantic content of the text. GNN-based models, despite being augmented with structural information (both content similarity and metadata), were unable to find additional useful signals to improve performance beyond the strong baseline set by a fine-tuned Transformer model like DistilBERT.

This suggests that the effectiveness of GNNs is highly contingent on the richness and quality of the structural information present in the data. In this case, the limited diversity of senders and domains in the dataset rendered the structural information insufficient to make a significant impact.

## How to Reproduce

1.  **Set up the Conda environment:**
    ```bash
    conda create -n bec_kangraph python=3.10 -y
    conda activate bec_kangraph
    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
    pip install pandas numpy scikit-learn matplotlib seaborn jupyter notebook transformers datasets tqdm torch_geometric
    # Install PyG extensions matching your CUDA/PyTorch version
    pip install pyg_lib torch_scatter torch_sparse torch_cluster -f https://data.pyg.org/whl/torch-2.5.0+cu121.html
    ```
2.  **Clone the repositories:**
    ```bash
    git clone https://github.com/ailabteam/bec_kangraph.git
    git clone https://github.com/r-dube/bec.git
    cd bec_kangraph
    ```
3.  **Run the scripts in order:**
    ```bash
    python 01_explore_data.py
    python 02_train_models.py
    python 03_train_kangraph.py
    python 04_build_hetero_graph.py
    python 05_train_hetero_kangraph.py
    ```
All results and artifacts will be saved in the `analysis_outputs` directory.

## Future Work

Our findings highlight the need for future BEC dataset development to include richer and more diverse metadata. This would enable a more thorough evaluation of the potential of graph-based detection methods and help drive research in this area forward.
